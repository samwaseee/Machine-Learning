{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d30f702",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Chapter 6: Building Logistic Regression from Scratch\n",
    "\n",
    "A Logistic Regression model is essentially a mathematical machine that takes in features (X), multiplies them by weights (W), adds a bias (b), and pushes the result through a \"Squashing Function\" to get a probability.\n",
    "\n",
    "### 1. The Prediction (The Sigmoid Function)\n",
    "\n",
    "Unlike Linear Regression, which can predict any number from negative to positive infinity, Logistic Regression must predict a probability between 0 and 1. We use the Sigmoid Function to achieve this:\n",
    "\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "Where  = X \\cdot W + b$. If $\\sigma(z)$ is 0.85, the model is 85% sure the passenger survived.\n",
    "\n",
    "### 2. The Penalty (Log Loss)\n",
    "\n",
    "How do we know if the model is doing a bad job? We use a Cost Function called Log Loss.\n",
    "\n",
    "*   If the passenger survived (1) but the model predicted 0.01, the \"Penalty\" is very high.\n",
    "*   If the model predicted 0.99, the \"Penalty\" is near zero.\n",
    "\n",
    "### 3. The Teacher (Gradient Descent)\n",
    "\n",
    "This is the \"learning\" part. The model calculates the slope (gradient) of the error and takes a small step in the opposite direction to reduce the penalty. It repeats this thousands of times until it finds the \"best\" weights for Pclass, Sex, and Age.\n",
    "\n",
    "#### Step 1: Initialize the Modeling Notebook\n",
    "\n",
    "In your new notebook, start by importing the clean data and setting up the mathematical foundations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ce61180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to train model with 891 samples and 9 features.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/titanic_model_ready.csv')\n",
    "\n",
    "# Separate features and target variable\n",
    "# We assume 'Survived' is the target variable and all other columns are features\n",
    "# So we add 'Survived' to the drop list for features and add it to the target variable\n",
    "# We add a column of 1s to X to handle the 'Bias' (b) term automatically\n",
    "X = df.drop('Survived', axis=1).astype(float).values\n",
    "y = df['Survived'].astype(float).values.reshape(-1, 1)\n",
    "\n",
    "# Initialize weights and bias\n",
    "weights = np.zeros((X.shape[1], 1))\n",
    "bias = 0.0\n",
    "\n",
    "print(f\"Ready to train model with {X.shape[0]} samples and {X.shape[1]} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e1b442",
   "metadata": {},
   "source": [
    "In this section, we build the three core functions of Logistic Regression: the Sigmoid (prediction), the Log Loss (error measurement), and Gradient Descent (learning).\n",
    "#### 1. The Sigmoid Function\n",
    "\n",
    "This function takes any real number and \"squashes\" it into a probability between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f4e736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc0a9e",
   "metadata": {},
   "source": [
    "#### The Cost Function (Log Loss)\n",
    "\n",
    "This measures how \"wrong\" the model's guesses are. A perfect prediction has a cost of 0, while a confident but wrong prediction has a very high cost.\n",
    "\n",
    "$$ J(W,b) = -\\frac{1}{m} \\sum [y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})] $$\n",
    "\n",
    "#### The Gradient Descent Algorithm\n",
    "\n",
    "This is the training loop. In each \"epoch\" (round of learning), the model:\n",
    "\n",
    "*   **Predicts:** Guesses survival probabilities.\n",
    "*   **Calculates Error:** Finds the difference between its guess and the real answer.\n",
    "*   **Updates Weights:** Adjusts the importance of features like Sex or Pclass to reduce future errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3fe71f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.6931\n",
      "Epoch 100: Loss = 0.6131\n",
      "Epoch 200: Loss = 0.5887\n",
      "Epoch 300: Loss = 0.5705\n",
      "Epoch 400: Loss = 0.5559\n",
      "Epoch 500: Loss = 0.5436\n",
      "Epoch 600: Loss = 0.5332\n",
      "Epoch 700: Loss = 0.5242\n",
      "Epoch 800: Loss = 0.5163\n",
      "Epoch 900: Loss = 0.5093\n",
      "\n",
      "--- Training Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "m = X.shape[0]  # Number of samples\n",
    "\n",
    "# Initial weights (9 features) and bias\n",
    "weights = np.zeros((X.shape[1], 1))\n",
    "bias = 0\n",
    "\n",
    "# The Training Loop\n",
    "for i in range(epochs):\n",
    "    # 1. Forward Pass: Calculate z and prediction (y_hat)\n",
    "    z = np.dot(X, weights) + bias\n",
    "    y_hat = sigmoid(z)\n",
    "    \n",
    "    # 2. Backward Pass: Calculate the Gradients (The Calculus)\n",
    "    dw = (1/m) * np.dot(X.T, (y_hat - y))\n",
    "    db = (1/m) * np.sum(y_hat - y)\n",
    "    \n",
    "    # 3. Update Weights and Bias (The Learning Step)\n",
    "    weights -= learning_rate * dw\n",
    "    bias -= learning_rate * db\n",
    "    \n",
    "    # Optional: Print progress every 100 epochs\n",
    "    if i % 100 == 0:\n",
    "        loss = -np.mean(y * np.log(y_hat + 1e-9) + (1 - y) * np.log(1 - y_hat + 1e-9))\n",
    "        print(f\"Epoch {i}: Loss = {loss:.4f}\")\n",
    "\n",
    "print(\"\\n--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d442c865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance:\n",
      "      Feature    Weight\n",
      "1  Sex_binary  0.921427\n",
      "6    HasCabin  0.308134\n",
      "3        Fare  0.295043\n",
      "8      Port_C  0.145387\n",
      "7      Port_S -0.107551\n",
      "4  FamilySize -0.133154\n",
      "5     IsAlone -0.180992\n",
      "2         Age -0.246106\n",
      "0      Pclass -0.379951\n"
     ]
    }
   ],
   "source": [
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': df.drop('Survived', axis=1).columns,\n",
    "    'Weight': weights.flatten()\n",
    "}).sort_values(by='Weight', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8a1cd3",
   "metadata": {},
   "source": [
    "*   **The Gender Dominance (Sex_binary: 0.92):** This is by far your strongest positive predictor. Because we mapped females to 1, this high positive weight confirms that being female was the single most significant factor in increasing survival probability.\n",
    "\n",
    "*   **The Status Proxy (HasCabin: 0.31 & Fare: 0.30):** These two features move together. Their positive weights suggest that having a recorded cabin and paying a higher fare significantly boosted survival odds, likely due to better lifeboat access.\n",
    "\n",
    "*   **The Class Penalty (Pclass: -0.38):** This is your strongest negative predictor. As the class number increases (from 1st to 3rd), the survival probability drops sharply. This mathematically captures the tragedy of the lower decks.\n",
    "\n",
    "*   **The Age Factor (Age: -0.25):** The negative weight suggests that, generally, as age increased, the chance of survival decreased. This aligns with the \"Children First\" priority we saw during our EDA.\n",
    "\n",
    "*   **Social Support (IsAlone: -0.18 & FamilySize: -0.13):** Interestingly, both carry negative weights. This suggests that being entirely alone or having a very large family size (which we standardized earlier) actually hindered survival compared to being in a small, cohesive family unit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3ab29b",
   "metadata": {},
   "source": [
    "#### Evaluating the Scratch Model\n",
    "\n",
    "To calculate accuracy, we need to convert the continuous output of our sigmoid function (which ranges from 0 to 1) into a binary output (0 or 1). We use a Threshold of 0.5:\n",
    "\n",
    "*   If the probability is â‰¥0.5, we predict Survived (1).\n",
    "*   If the probability is <0.5, we predict Not Survived (0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01760141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Correct Predictions: 681 out of 891\n",
      "\n",
      "Training Accuracy: 76.43%\n"
     ]
    }
   ],
   "source": [
    "def predict(X, weights, bias):\n",
    "    z = np.dot(X, weights) + bias\n",
    "    probabilities = sigmoid(z)\n",
    "    return [1 if p >= 0.5 else 0 for p in probabilities]\n",
    "\n",
    "# Make predictions on the training set\n",
    "predictions = predict(X, weights, bias)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = np.sum(predictions == y.flatten())\n",
    "accuracy = np.mean(correct_predictions / len(y)) * 100\n",
    "\n",
    "print(f\"Total Correct Predictions: {correct_predictions} out of {len(y)}\")\n",
    "print(f\"\\nTraining Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4ca2e9",
   "metadata": {},
   "source": [
    "### The Professional Baseline (Scikit-Learn)\n",
    "\n",
    "We will now implement LogisticRegression from the sklearn library to see if the optimized algorithms can improve upon our 76.43% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2d4d1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sklearn Logistic Regression Training Accuracy: 80.58%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "sklearn_model = LogisticRegression(max_iter=1000)\n",
    "sklearn_model.fit(X, y.ravel()) # .ravel() flattens y to 1D array for sklearn\n",
    "\n",
    "# Make predictions\n",
    "sklearn_predictions = sklearn_model.predict(X)\n",
    "\n",
    "# Calculate accuracy\n",
    "sklearn_accuracy = accuracy_score(y, sklearn_predictions) * 100\n",
    "\n",
    "print(f\"\\nSklearn Logistic Regression Training Accuracy: {sklearn_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "65ce11fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Feature  Scratch_Weight  Sklearn_Weight\n",
      "1  Sex_binary        0.921427        2.572020\n",
      "6    HasCabin        0.308134        0.625443\n",
      "3        Fare        0.295043        0.060970\n",
      "8      Port_C        0.145387        0.041350\n",
      "7      Port_S       -0.107551       -0.297884\n",
      "2         Age       -0.246106       -0.488666\n",
      "4  FamilySize       -0.133154       -0.582355\n",
      "5     IsAlone       -0.180992       -0.637918\n",
      "0      Pclass       -0.379951       -0.818242\n"
     ]
    }
   ],
   "source": [
    "# Compare weights side-by-side\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Feature': df.drop('Survived', axis=1).columns,\n",
    "    'Scratch_Weight': weights.flatten(),\n",
    "    'Sklearn_Weight': sklearn_model.coef_.flatten()\n",
    "}).sort_values(by='Sklearn_Weight', ascending=False)\n",
    "\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fdb781ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Model Health Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Died       0.83      0.85      0.84       549\n",
      "    Survived       0.76      0.73      0.74       342\n",
      "\n",
      "    accuracy                           0.81       891\n",
      "   macro avg       0.80      0.79      0.79       891\n",
      "weighted avg       0.80      0.81      0.81       891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate the professional report\n",
    "report = classification_report(y, sklearn_predictions, target_names=['Died', 'Survived'])\n",
    "print(\"--- Final Model Health Report ---\")\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
