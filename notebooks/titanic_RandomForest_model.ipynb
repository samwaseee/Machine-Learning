{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1da9d424",
   "metadata": {},
   "source": [
    "## Chapter 10: The \"Forest of Decisions\"\n",
    "\n",
    "Our next model will be the Random Forest. If Logistic Regression is like a judge weighing evidence on a scale, a Random Forest is like a committee of experts asking a series of \"Yes/No\" questions.\n",
    "### 10.1 Why move to Random Forest?\n",
    "\n",
    "Logistic Regression has a \"Linear Constraint.\" It assumes that if a feature is important, its impact is consistent. But survival on the Titanic was full of interactions:\n",
    "\n",
    "    Interaction Example: Being a child was good for survival, but being a child in 3rd Class was very different from being a child in 1st Class.\n",
    "\n",
    "    The Random Forest Solution: It can create branches like: If Pclass is 3 AND Age < 10, then check Fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "504ed631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to train! Training on 712 samples, testing on 179.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Load the data we cleaned in the first notebook\n",
    "df = pd.read_csv('../data/titanic_model_ready.csv')\n",
    "\n",
    "# 2. Prepare X and y\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "# 3. The \"Golden Split\" (Using 42 again so results are comparable)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Ready to train! Training on {len(X_train)} samples, testing on {len(X_test)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7060ba8e",
   "metadata": {},
   "source": [
    "A Random Forest is just a collection of Decision Trees. To build the forest, we first have to build a single tree that can think for itself.\n",
    "\n",
    "### 1.1 The Logic: What is \"Gini Impurity\"?\n",
    "\n",
    "In Logistic Regression, we used Log Loss to measure error. In Decision Trees, we use Gini Impurity. It measures how \"mixed\" a group is.\n",
    "\n",
    "*   **Pure Group:** 100% Survivors (Gini = 0).\n",
    "*   **Mixed Group:** 50% Survivors, 50% Died (Gini = 0.5 — This is \"Chaos\").\n",
    "\n",
    "The goal of our tree is to ask questions that reduce the Chaos the most.\n",
    "\n",
    "### 1.2 The Building Blocks (The \"Node\")\n",
    "\n",
    "First, we need a structure to hold our questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cecaea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature       # Which column are we looking at? (e.g., Sex)\n",
    "        self.threshold = threshold   # At what value do we split? (e.g., 0.5)\n",
    "        self.left = left             # The \"Yes\" branch\n",
    "        self.right = right           # The \"No\" branch\n",
    "        self.value = value           # If it's a leaf, what's the answer? (Survived/Died)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07c1c15",
   "metadata": {},
   "source": [
    "### The \"Chaos\" Math ($1 - \\sum p^2$)\n",
    "\n",
    "This is the Gini Impurity formula. It measures how often a randomly chosen element from the set would be incorrectly labeled.\n",
    "\n",
    "$$ Gini = 1 - (p_{died}^2 + p_{survived}^2) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "732d8235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gini(y):\n",
    "    m = len(y)\n",
    "    if m == 0: return 0\n",
    "    p1 = np.sum(y) / m  # Probability of surviving\n",
    "    p0 = 1 - p1         # Probability of dying\n",
    "    return 1 - (p0**2 + p1**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6d34cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_split(X, y):\n",
    "    best_gini = 1.0 # Initialize with worst Gini\n",
    "    split_idx, split_thresh = None, None # winning Feature (column index) and the winning Threshold (the number we split on)\n",
    "    \n",
    "    for feature_idx in range(X.shape[1]): # loop through each feature/column\n",
    "        thresholds = np.unique(X[:, feature_idx]) # get all unique values in this column\n",
    "        for thresh in thresholds: # loop through each unique value\n",
    "            # Try splitting the data here \n",
    "            left_idxs = np.where(X[:, feature_idx] <= thresh)[0] # if value is less than or equal to threshold\n",
    "            right_idxs = np.where(X[:, feature_idx] > thresh)[0]\n",
    "            \n",
    "            if len(left_idxs) == 0 or len(right_idxs) == 0: continue\n",
    "            \n",
    "            # Calculate weighted Gini\n",
    "            g_left = calculate_gini(y[left_idxs])\n",
    "            g_right = calculate_gini(y[right_idxs])\n",
    "            n, n_l, n_r = len(y), len(left_idxs), len(right_idxs)\n",
    "            weighted_gini = (n_l/n) * g_left + (n_r/n) * g_right\n",
    "            \n",
    "            if weighted_gini < best_gini:\n",
    "                best_gini = weighted_gini\n",
    "                split_idx = feature_idx\n",
    "                split_thresh = thresh\n",
    "                \n",
    "    return split_idx, split_thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff32698c",
   "metadata": {},
   "source": [
    "### 1.5 The Strategy for the Forest\n",
    "\n",
    "Once we have a single tree working, the \"Random Forest\" part is actually quite simple:\n",
    "\n",
    "    Bootstrapping: Give each tree a random 80% of the data.\n",
    "\n",
    "    Feature Randomness: Each tree is only allowed to look at a few random features (e.g., it can't see \"Sex\" so it has to find other patterns).\n",
    "\n",
    "    Voting: Ask all trees for their opinion and take the majority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6873f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDecisionTree:\n",
    "    def __init__(self, max_depth=5):\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        num_samples, num_features = X.shape\n",
    "        num_labels = len(np.unique(y))\n",
    "\n",
    "        # Stopping Criteria:\n",
    "        # 1. We reached max depth\n",
    "        # 2. There is only one class left (Pure)\n",
    "        # 3. No more samples\n",
    "        if depth >= self.max_depth or num_labels == 1 or num_samples < 2:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        # Find the best split using the logic from our previous step\n",
    "        feat_idx, thresh = best_split(X, y)\n",
    "\n",
    "        if feat_idx is None:\n",
    "            return Node(value=self._most_common_label(y))\n",
    "\n",
    "        # Grow the branches\n",
    "        left_idxs = np.where(X[:, feat_idx] <= thresh)[0]\n",
    "        right_idxs = np.where(X[:, feat_idx] > thresh)[0]\n",
    "\n",
    "        left = self._build_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right = self._build_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "\n",
    "        return Node(feature=feat_idx, threshold=thresh, left=left, right=right)\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        return np.round(np.mean(y)) # For binary 0/1, the mean rounded is the majority\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "    \n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.value is not None:\n",
    "                return node.value\n",
    "    \n",
    "        if x[node.feature] <= node.threshold:\n",
    "                return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bf34cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scratch Single Tree Accuracy: 79.89%\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize and Train\n",
    "my_tree = ScratchDecisionTree(max_depth=3)\n",
    "my_tree.fit(X_train.values, y_train.values)\n",
    "\n",
    "# 2. Predict on the Test Set\n",
    "y_tree_pred = my_tree.predict(X_test.values)\n",
    "\n",
    "# 3. Check Accuracy\n",
    "tree_acc = np.mean(y_tree_pred == y_test.values)\n",
    "print(f\"Scratch Single Tree Accuracy: {tree_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7491bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchRandomForest:\n",
    "    def __init__(self, n_trees=10, max_depth=5, sample_sz=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.sample_sz = sample_sz\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        n_samples = X.shape[0]\n",
    "        if self.sample_sz is None: self.sample_sz = n_samples\n",
    "        \n",
    "        for _ in range(self.n_trees):\n",
    "            # 1. Create a Bootstrap Sample (Randomly pick rows with replacement)\n",
    "            indices = np.random.choice(n_samples, self.sample_sz, replace=True)\n",
    "            X_sample, y_sample = X[indices], y[indices]\n",
    "            \n",
    "            # 2. Train a tree on this random sample\n",
    "            tree = ScratchDecisionTree(max_depth=self.max_depth)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # 3. Get predictions from all trees\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        \n",
    "        # 4. Majority Vote: Average the predictions and round to 0 or 1\n",
    "        # (axis=0 means we average vertically across the trees)\n",
    "        avg_preds = np.mean(tree_preds, axis=0)\n",
    "        return (avg_preds >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3e9dd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scratch Random Forest Accuracy: 78.21%\n"
     ]
    }
   ],
   "source": [
    "# Initialize our ensemble\n",
    "my_forest = ScratchRandomForest(n_trees=10, max_depth=5)\n",
    "\n",
    "# Train (This will take a few seconds because it builds 10 trees!)\n",
    "my_forest.fit(X_train.values, y_train.values)\n",
    "\n",
    "# Predict\n",
    "y_forest_pred = my_forest.predict(X_test.values)\n",
    "\n",
    "# Final Accuracy\n",
    "forest_acc = np.mean(y_forest_pred == y_test.values.flatten())\n",
    "print(f\"Scratch Random Forest Accuracy: {forest_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853d694e",
   "metadata": {},
   "source": [
    "### 1.3 The Theory: What is a Random Forest?\n",
    "\n",
    "Imagine you are trying to decide if a passenger survived.\n",
    "\n",
    "*   **A Decision Tree asks:** \"Is it a woman?\" → \"Yes\" → \"Is she in 1st class?\" → \"Yes\" → Survived.\n",
    "*   **A Random Forest** is a collection of 100 different trees. Each tree gets to see a slightly different version of the data.\n",
    "*   **The Final Answer:** All 100 trees \"vote.\" If 80 trees say \"Survived\" and 20 say \"Died,\" the forest predicts Survived.\n",
    "\n",
    "### 1.4 Training the \"Forest\"\n",
    "\n",
    "Here is the code to kick off your new notebook's first model. We will start with a \"Constrained Forest\" (max_depth=5) to make sure it doesn't just memorize the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f47b9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.88826815642457 %\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=5)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred)*100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
